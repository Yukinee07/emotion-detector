# -*- coding: utf-8 -*-
"""Project_faceAndEmotionDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OvA5j66II0X3FVnja7aiAQ8j5Y6vj3uo

# Training the Emotion Classifier Model

Uploading the Dataset
"""

from google.colab import files
import zipfile
import os

uploaded = files.upload()
with zipfile.ZipFile('archive.zip', 'r') as zip_ref:
    zip_ref.extractall('images')

"""Importing Libraries"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from PIL import Image

"""Data Preparation"""

# Set random seeds for reproducible results
tf.random.set_seed(42)
np.random.seed(42)

# Setting the dataset path here
DATASET_PATH = "images/train"

# Setting emotion folder names
EMOTION_FOLDERS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']

def load_images_from_folders(dataset_path, emotion_folders, target_size=(48, 48)):
    images = []
    labels = []

    print(f"Loading images from: {dataset_path}")

    for emotion_idx, emotion_folder in enumerate(emotion_folders):
        emotion_path = os.path.join(dataset_path, emotion_folder)

        if not os.path.exists(emotion_path):
            print(f"Warning: Folder '{emotion_folder}' not found. Skipping...")
            continue

        print(f"Loading {emotion_folder} images...")
        image_count = 0

        # Get all image files in the folder
        for filename in os.listdir(emotion_path):
            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                image_path = os.path.join(emotion_path, filename)

                try:
                    # Load and preprocess image
                    img = Image.open(image_path)

                    # Convert to grayscale if needed
                    if img.mode != 'L':
                        img = img.convert('L')

                    # Resize image
                    img = img.resize(target_size)

                    # Convert to numpy array and normalize
                    img_array = np.array(img) / 255.0

                    images.append(img_array)
                    labels.append(emotion_idx)
                    image_count += 1

                except Exception as e:
                    print(f"Error loading {image_path}: {e}")
                    continue

        print(f"  Loaded {image_count} images from {emotion_folder}")

    images = np.array(images)
    labels = np.array(labels)

    # Add channel dimension for CNN (grayscale = 1 channel)
    images = images.reshape(-1, target_size[0], target_size[1], 1)

    print(f"\nTotal images loaded: {len(images)}")
    print(f"Image shape: {images.shape}")
    print(f"Unique emotions: {len(np.unique(labels))}")

    return images, labels

def auto_detect_folders(dataset_path):
    """
    Automatically detect emotion folders in the dataset path
    """
    if not os.path.exists(dataset_path):
        return None

    folders = [f for f in os.listdir(dataset_path)
              if os.path.isdir(os.path.join(dataset_path, f))]

    print(f"Found folders: {folders}")
    return sorted(folders)

# Try to auto-detect folders first
if os.path.exists(DATASET_PATH):
    detected_folders = auto_detect_folders(DATASET_PATH)
    if detected_folders:
        print(f"Auto-detected emotion folders: {detected_folders}")
        EMOTION_FOLDERS = detected_folders

# Load the data
images, emotions = load_images_from_folders(DATASET_PATH, EMOTION_FOLDERS)

# Update emotion labels to match the actual folders
emotion_labels = [folder.capitalize() for folder in EMOTION_FOLDERS]
print(f"Emotion labels: {emotion_labels}")

"""Loading Training and Validation Sets"""

# Split data: 80% train, 10% validation, 10% test
from sklearn.model_selection import train_test_split

# Check if we have enough data for each class
unique, counts = np.unique(emotions, return_counts=True)
min_samples = min(counts)

if min_samples < 10:
    print(f"Warning: Some emotions have very few samples (min: {min_samples})")

X_temp, X_test, y_temp, y_test = train_test_split(
    images, emotions, test_size=0.10, random_state=42,
    stratify=emotions if min_samples >= 2 else None
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.111, random_state=42,
    stratify=y_temp if min_samples >= 3 else None
)

print(f"Training set: {len(X_train)} images")
print(f"Validation set: {len(X_val)} images")
print(f"Test set: {len(X_test)} images")

# Convert labels to categorical (one-hot encoding)
num_classes = len(emotion_labels)
y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)
y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes)
y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)

"""Creating the CNN Model"""

def create_cnn(input_shape, num_classes):

    model = tf.keras.Sequential([
        # Module 1
        tf.keras.layers.Conv2D(256, (3, 3), input_shape=input_shape, data_format='channels_last'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        tf.keras.layers.Conv2D(256, (3, 3), padding='same'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.MaxPooling2D(2, 2),

        # Module 2
        tf.keras.layers.Conv2D(128, (3, 3), padding='same'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        tf.keras.layers.Conv2D(128, (3, 3), padding='same'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.MaxPooling2D(2, 2),

        # Module 3
        tf.keras.layers.Conv2D(64, (3, 3), padding='same'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        tf.keras.layers.Conv2D(64, (3, 3), padding='same'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.MaxPooling2D(2, 2),

        # Flatten
        tf.keras.layers.Flatten(),

        # Dense layers
        tf.keras.layers.Dense(512),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        tf.keras.layers.Dropout(0.5),

        tf.keras.layers.Dense(256),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        tf.keras.layers.Dropout(0.5),

        tf.keras.layers.Dense(128),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        tf.keras.layers.Dropout(0.5),

        # Output layer
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])

    return model

input_shape = X_train.shape[1:]
num_classes = len(emotion_labels)

# Create the model
model = create_cnn(input_shape, num_classes)

# Display model architecture
print("\nModel Architecture:")
model.summary()

# Compiling the model
model.compile(
    optimizer='adam',                         # Adam optimizer works well out of the box
    loss='categorical_crossentropy',
    metrics=['accuracy']                      # Track accuracy during training
)

"""Data Augmentation"""

# Create data augmentation generator
datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=10,          # Rotate images randomly by up to 10 degrees
    width_shift_range=0.1,      # Shift images horizontally by up to 10%
    height_shift_range=0.1,     # Shift images vertically by up to 10%
    horizontal_flip=True,       # Randomly flip images horizontally
    zoom_range=0.1,             # Randomly zoom in/out by up to 10%
    fill_mode='nearest'         # Fill in new pixels with nearest pixel values
)

# Fit the augmentation generator on training data
datagen.fit(X_train)

"""Training the Model"""

# Setting up callbacks for better training
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,                 # Stop training if validation loss doesn't improve for 5 epochs
        restore_best_weights=True   # Restore the best weights when stopping
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,                 # Reduce learning rate by half
        patience=3,                 # Reduce LR if no improvement for 3 epochs
        min_lr=0.0001               # Don't reduce LR below this value
    )
]

#Train the model
history = model.fit(
    datagen.flow(X_train, y_train_cat, batch_size=32),
    epochs=25,
    validation_data=(X_val, y_val_cat),
    callbacks=callbacks,
    verbose=1
)

"""Saving the Model"""

model.save('emotion_model.keras')
files.download('emotion_model.keras')

"""Model Evaluation"""

# Evaluate on test set
test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)
print(f"\nTest Accuracy: {test_accuracy:.4f}")
print(f"Test Loss: {test_loss:.4f}")

# Make predictions on test set
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Generate classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_classes, target_names=emotion_labels))


# Plot training history
plt.figure(figsize=(15, 5))

# Plot training & validation accuracy
plt.subplot(1, 3, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training & validation loss
plt.subplot(1, 3, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot confusion matrix
plt.subplot(1, 3, 3)
cm = confusion_matrix(y_test, y_pred_classes)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=emotion_labels, yticklabels=emotion_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.tight_layout()
plt.show()

"""Testing on Individual Images"""

if len(X_test) > 0:
    num_test_images = min(10, len(X_test))
    test_indices = np.random.choice(len(X_test), num_test_images, replace=False)

    plt.figure(figsize=(15, 3))
    for i, idx in enumerate(test_indices):
        image = X_test[idx]
        true_emotion = emotion_labels[y_test[idx]]

        # Make prediction
        pred = model.predict(image.reshape(1, *image.shape), verbose=0)
        predicted_emotion = emotion_labels[np.argmax(pred)]
        confidence = np.max(pred)

        plt.subplot(1, num_test_images, i+1)
        plt.imshow(image.reshape(input_shape[0], input_shape[1]), cmap='gray')
        plt.title(f'True: {true_emotion}\nPred: {predicted_emotion}\nConf: {confidence:.2f}')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# Load and predict on new image
def predict_emotion(image_path):
    img = Image.open(image_path)
    img = img.convert('L')  # Convert to grayscale
    img = img.resize((48, 48))  # Resize to model input size
    img_array = np.array(img) / 255.0
    img_array = img_array.reshape(1, 48, 48, 1)

    prediction = model.predict(img_array)
    emotion_index = np.argmax(prediction)
    confidence = prediction[0][emotion_index]

    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
    return emotions[emotion_index], confidence

# Use it
emotion, confidence = predict_emotion('images/test/happy/PrivateTest_3524293.jpg')
print(f"Predicted emotion: {emotion} (confidence: {confidence:.2f})")

"""# Facial Detection and Emotion Classification Using Test Cases

Installing Pre Trained Facial Detection Model
"""

!pip install mtcnn

"""Loading Our Trained Emotion Detection Model"""

from google.colab import files
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from PIL import Image, ImageDraw, ImageFont
from mtcnn.mtcnn import MTCNN
import cv2

# Load the emotion classifier model
uploaded = files.upload()  # Select the `emotion_model.keras` file from your laptop
model = tf.keras.models.load_model('emotion_model.keras')

# Show the model architecture
model.summary()

emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']

# Initialize face detector
detector = MTCNN()

"""Detecting and Predicting Emotions of People from Uploaded Images"""

def preprocess_face(face_img):
    face_img = face_img.convert('L')  # Grayscale
    face_img = face_img.resize((48, 48))
    face_array = np.array(face_img) / 255.0
    face_array = face_array.reshape(1, 48, 48, 1)
    return face_array

def predict_emotion(face_array):
    prediction = model.predict(face_array)
    emotion_index = np.argmax(prediction)
    confidence = prediction[0][emotion_index]
    return emotions[emotion_index], confidence

def process_image(image_path):
    image = Image.open(image_path).convert('RGB')
    image_np = np.array(image)
    detections = detector.detect_faces(image_np)

    faces = []
    preprocessed_faces = []
    results = []

    draw = ImageDraw.Draw(image)

    for det in detections:
        x, y, w, h = det['box']
        x, y = max(0, x), max(0, y)

        # Crop and process the face
        face_crop = image.crop((x, y, x + w, y + h))
        preprocessed = preprocess_face(face_crop)
        emotion, confidence = predict_emotion(preprocessed)

        faces.append(face_crop)
        preprocessed_faces.append(Image.fromarray((preprocessed[0, :, :, 0] * 255).astype('uint8')))
        results.append((x, y, w, h, emotion, confidence))

        # Draw bounding box and label
        font = ImageFont.truetype("LiberationSansNarrow-Bold.ttf", size=22)
        draw.rectangle([x, y, x + w, y + h], outline='green', width=2)
        draw.text((x, y - 20), f"{emotion} ({confidence:.2f})", fill='red', font=font)

    # Display results
    plt.figure(figsize=(15, 10))

    # Show each preprocessed face with its predicted emotion
    for i, (face, processed, (x, y, w, h, emotion, conf)) in enumerate(zip(faces, preprocessed_faces, results)):
        plt.subplot(2, len(faces), len(faces) + i + 1)
        plt.title(f"{emotion} ({conf:.2f})")
        plt.imshow(processed, cmap='gray')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

    # Original image with bounding boxes
    plt.figure(figsize=(10, 8))
    plt.title("Original Image with Bounding Boxes")
    plt.imshow(image)
    plt.axis('off')

# Example use
testCaseImage = list(files.upload().keys())[0]

plt.figure(figsize=(10, 8))
plt.title("Original Image")
plt.imshow(Image.open(testCaseImage))
plt.axis('off')

process_image(testCaseImage)

"""# Deployment (YOU WILL NEED TO RUN THIS LOCALLY)"""

import cv2
import numpy as np
from mtcnn.mtcnn import MTCNN
import tensorflow as tf

# Loading the trained model
model = tf.keras.models.load_model('emotion_model.keras')
emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']

# Initialize MTCNN detector
detector = MTCNN()

def preprocess_face_img(face_img):
    gray = cv2.cvtColor(face_img, cv2.COLOR_RGB2GRAY)
    resized = cv2.resize(gray, (48, 48))
    norm = resized / 255.0
    reshaped = norm.reshape(1, 48, 48, 1)
    return reshaped

def predict_emotion(face_array):
    preds = model.predict(face_array, verbose=0)
    emotion_index = np.argmax(preds)
    confidence = preds[0][emotion_index]
    return emotions[emotion_index], confidence

# Webcam
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

frame_count = 0
previous_results = []

print("Press 'q' to exit the real-time emotion detection.")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame = cv2.flip(frame, 1)

    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    if frame_count % 3 == 0:
        # Detection and prediction
        faces = detector.detect_faces(rgb_frame)
        new_results = []

        for face in faces:
            x, y, w, h = face['box']
            x, y = max(0, x), max(0, y)
            face_crop = rgb_frame[y:y+h, x:x+w]

            if face_crop.size == 0:
                continue

            face_array = preprocess_face_img(face_crop)
            emotion, confidence = predict_emotion(face_array)
            new_results.append(((x, y, w, h), f"{emotion} ({confidence:.2f})"))

        previous_results = new_results

    for (x, y, w, h), label in previous_results:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,
                    0.8, (0, 0, 255), 2)

    cv2.imshow('Live Emotion Detection', frame)
    frame_count += 1

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()